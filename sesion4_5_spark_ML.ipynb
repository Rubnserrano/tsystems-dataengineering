{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e87333-7501-4ba1-b8dd-c66ff808a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"T-systems: cuarta sesion. MLib\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc73200-209c-40ab-adaa-3472c5066d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"C:\\\\Users\\\\ruben\\\\Desktop\\\\data engineering\\\\s5-pysparkML\\\\results.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55a58bb-caa7-4635-995a-797ae1371060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----+----+----+----+-------+-----+\n",
      "|Question 1|Question 2|Question 3|Question 4|Ex01|Ex02|Ex03|Ex04|Project|Total|\n",
      "+----------+----------+----------+----------+----+----+----+----+-------+-----+\n",
      "|        18|        16|        23|        22| 100|  85|  80|  70|     80|   81|\n",
      "|        20|        13|        11|        22| 100|  85|  80|  90|     93|   79|\n",
      "|         7|         7|         0|         2| 100| 100|  85|  30|     70|   42|\n",
      "|        14|        16|         4|        13|  95|  95| 100|  55|     87|   67|\n",
      "|         4|         6|         0|         0|  65|  95|  65|  25|     70|   39|\n",
      "+----------+----------+----------+----------+----+----+----+----+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f488e7a4-078d-4bdd-b9f4-d66903de194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predecir total con ex01,..4, project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3b2e5f-c879-4544-a3f9-3526f5e3e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sql_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa0708e0-1a99-4e7a-a18f-ef0200a44522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Question 1: string (nullable = true)\n",
      " |-- Question 2: string (nullable = true)\n",
      " |-- Question 3: string (nullable = true)\n",
      " |-- Question 4: string (nullable = true)\n",
      " |-- Ex01: string (nullable = true)\n",
      " |-- Ex02: string (nullable = true)\n",
      " |-- Ex03: string (nullable = true)\n",
      " |-- Ex04: string (nullable = true)\n",
      " |-- Project: string (nullable = true)\n",
      " |-- Total: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f2f3f5-0223-472a-b950-f953641d5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select([sql_f.col(c).cast('float') for c in ['Ex01','Ex02','Ex03','Ex04','Project','Total']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "587787ee-5abf-4490-bf23-93d9193049fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ex01: float (nullable = true)\n",
      " |-- Ex02: float (nullable = true)\n",
      " |-- Ex03: float (nullable = true)\n",
      " |-- Ex04: float (nullable = true)\n",
      " |-- Project: float (nullable = true)\n",
      " |-- Total: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1004b4-d605-4b0f-83b6-fc49018569cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed = 12345678 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d1fb71d-eef1-4658-8b7b-3426e08ba308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 22)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count(), test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a358bd1-727b-471c-b1cc-50b014fe5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52875801-e52f-43ca-973d-909150d48694",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter = 10, regParam = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dc9ce82-39a8-4e53-a8b9-a4b102849f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.LinearRegression"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f0b1c3-0f76-4c62-a8f1-0bf981f79ca6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: Ex01, Ex02, Ex03, Ex04, Project, Total",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: features does not exist. Available: Ex01, Ex02, Ex03, Ex04, Project, Total"
     ]
    }
   ],
   "source": [
    "model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "247e19c0-ccea-491f-b620-fdf171433adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "simple_training =  spark.createDataFrame([\n",
    "    (Vectors.dense([0.0, 1.1, 0.1]), 1.0),\n",
    "    (Vectors.dense([0.0, 1.1, 0.1]), 1.0),\n",
    "    (Vectors.dense([0.0, 1.1, 0.1]), 1.0),\n",
    "    (Vectors.dense([0.0, 1.1, 0.1]), 1.0)], ['features', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ce077d3-4382-4987-9309-1fe0b77d4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(simple_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69f2c5f6-e817-4680-8eb1-0d87549a8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_test =  spark.createDataFrame([\n",
    "    (Vectors.dense([0.0, 1.1, 0.1]), 1.0),\n",
    "    (Vectors.dense([0.0, 1.1, 0.1]), 1.0),\n",
    "    (Vectors.dense([0.0, 1.1, 0.1]), 1.0),\n",
    "    (Vectors.dense([0.0, 1.1, 0.1]), 1.0)], ['features', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64153c47-7bda-4933-b672-d026189d5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.transform(simple_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "214ac548-5fc5-43de-8321-40d0a2a638f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+\n",
      "|     features|label|prediction|\n",
      "+-------------+-----+----------+\n",
      "|[0.0,1.1,0.1]|  1.0|       1.0|\n",
      "|[0.0,1.1,0.1]|  1.0|       1.0|\n",
      "|[0.0,1.1,0.1]|  1.0|       1.0|\n",
      "|[0.0,1.1,0.1]|  1.0|       1.0|\n",
      "+-------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show() #el predict te crea una nueva columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75f1fc7b-ae9c-45a2-a252-5df1f22a13e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+-------+-----+\n",
      "|Ex01|Ex02|Ex03|Ex04|Project|Total|\n",
      "+----+----+----+----+-------+-----+\n",
      "|60.0|80.0|70.0|40.0|    0.0| 26.0|\n",
      "|60.0|85.0|60.0|20.0|   70.0| 48.0|\n",
      "+----+----+----+----+-------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbb22fc4-0bdc-43ee-9b43-36d4545df3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b5a9ba9-3d09-4e71-9c67-1368f0e98dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = ['Ex01', 'Ex02', 'Ex03', 'Ex04', 'Project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7257515a-a757-4e96-9fc5-aaf659236c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = columnas, outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96ebc123-4af1-4f31-be05-0473159903a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = assembler.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6886ab39-0469-4ab0-8165-be16f12adb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+-------+-----+--------------------------+\n",
      "|Ex01|Ex02|Ex03|Ex04|Project|Total|features                  |\n",
      "+----+----+----+----+-------+-----+--------------------------+\n",
      "|60.0|80.0|70.0|40.0|0.0    |26.0 |[60.0,80.0,70.0,40.0,0.0] |\n",
      "|60.0|85.0|60.0|20.0|70.0   |48.0 |[60.0,85.0,60.0,20.0,70.0]|\n",
      "|65.0|90.0|90.0|85.0|69.0   |65.0 |[65.0,90.0,90.0,85.0,69.0]|\n",
      "+----+----+----+----+-------+-----+--------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(3, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f58a6262-5146-45c3-97ca-1a1d20649770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_16a5e990f186"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.setLabelCol(\"Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "064dd93c-75f6-461d-bba9-9afc92f5fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41d37551-39ab-4083-9cbc-3c02d20dada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-----+-------+-----+---------------------------+-----------------+\n",
      "|Ex01|Ex02|Ex03 |Ex04 |Project|Total|features                   |prediction       |\n",
      "+----+----+-----+-----+-------+-----+---------------------------+-----------------+\n",
      "|60.0|85.0|75.0 |85.0 |70.0   |53.0 |[60.0,85.0,75.0,85.0,70.0] |63.28826648908446|\n",
      "|70.0|85.0|65.0 |100.0|65.0   |64.0 |[70.0,85.0,65.0,100.0,65.0]|66.04627665225328|\n",
      "|80.0|80.0|65.0 |60.0 |55.0   |62.0 |[80.0,80.0,65.0,60.0,55.0] |54.0656664622132 |\n",
      "|80.0|85.0|100.0|80.0 |92.0   |81.0 |[80.0,85.0,100.0,80.0,92.0]|76.519676196518  |\n",
      "|80.0|90.0|90.0 |90.0 |67.0   |42.0 |[80.0,90.0,90.0,90.0,67.0] |68.23885473124211|\n",
      "+----+----+-----+-----+-------+-----+---------------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.transform(assembler.transform(test_df))\n",
    "pred.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4f8240c-8590-42e5-9de9-4e633e2a9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ce0f7d8-ce28-4990-ab29-7b46701d4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [assembler, lr])\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "053ff885-fa86-4ab3-ab59-7bfc6a47512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10717bb0-20c3-4d94-ba78-ea5c1737243a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-----+-------+-----+---------------------------+-----------------+\n",
      "|Ex01|Ex02|Ex03 |Ex04 |Project|Total|features                   |prediction       |\n",
      "+----+----+-----+-----+-------+-----+---------------------------+-----------------+\n",
      "|60.0|85.0|75.0 |85.0 |70.0   |53.0 |[60.0,85.0,75.0,85.0,70.0] |63.28826648908446|\n",
      "|70.0|85.0|65.0 |100.0|65.0   |64.0 |[70.0,85.0,65.0,100.0,65.0]|66.04627665225328|\n",
      "|80.0|80.0|65.0 |60.0 |55.0   |62.0 |[80.0,80.0,65.0,60.0,55.0] |54.0656664622132 |\n",
      "|80.0|85.0|100.0|80.0 |92.0   |81.0 |[80.0,85.0,100.0,80.0,92.0]|76.519676196518  |\n",
      "|80.0|90.0|90.0 |90.0 |67.0   |42.0 |[80.0,90.0,90.0,90.0,67.0] |68.23885473124211|\n",
      "+----+----+-----+-----+-------+-----+---------------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfe034b8-0df0-4f78-9c1b-9893195bba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|       prediction|Total|\n",
      "+-----------------+-----+\n",
      "|63.28826648908446| 53.0|\n",
      "|66.04627665225328| 64.0|\n",
      "| 54.0656664622132| 62.0|\n",
      "|  76.519676196518| 81.0|\n",
      "|68.23885473124211| 42.0|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.select(\"prediction\", \"Total\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ea60269-117e-457d-b7c5-193f973c4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bc21a00-6ab3-44c7-bee3-d5eefde6563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_evaluator = RegressionEvaluator(metricName= \"rmse\",\n",
    "                                     labelCol = \"Total\", \n",
    "                                     predictionCol = \"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc87682-26fb-434b-a60e-cf809b887dca",
   "metadata": {},
   "source": [
    "### calcular rmse, sql_f.pow, sql_f.mean, sql_f.sqrt, agg()..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00ffca46-f113-48b6-bce9-999d561a6fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.909428387895298"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25508fca-62d5-42f3-a0b8-08c1304b4c7f",
   "metadata": {},
   "source": [
    "### ampliar pipeline con StandardScaler.  -> assembler, standardscaler, lr + error y cual funciona mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7446dde-0f6f-45a8-a8a1-c81fbc4b6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cec171d0-cbce-4f35-aa85-1fcaa9079afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_evaluator = RegressionEvaluator(metricName = 'rmse', labelCol = 'Total',\n",
    "                                     predictionCol = 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "606495b8-bbe5-49a8-a438-9cd31a965c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.909428387895298"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a53eeb8-bd29-45d3-97e4-0f069db3ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "444bbbef-b9e4-4932-971d-cedc7401ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.5, 0.1]) \\\n",
    "    .addGrid(lr.maxIter, [1,5,10,20]) \\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0d3a06a-3d1b-488d-8116-1093600730e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(\n",
    "    estimator = pipeline,\n",
    "    estimatorParamMaps = paramGrid,\n",
    "    evaluator = rmse_evaluator,\n",
    "    numFolds = 3,\n",
    "    seed = 12345678,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39a27d74-32b6-417a-837c-4da429ca469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = crossval.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f0f9650-4be4-427d-8e86-e62bf9c40143",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cv_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac16cee8-7cf2-41f8-897f-8d223e090604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.905107498598843"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d48987e-f170-4bd4-b534-5cb640a14934",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestLRModel = cv_model.bestModel.stages[1]\n",
    "bestParams = bestLRModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a9be3872-2709-4ac7-94f4-db35af744df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LinearRegression_16a5e990f186', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LinearRegression_16a5e990f186', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LinearRegression_16a5e990f186', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35,\n",
       " Param(parent='LinearRegression_16a5e990f186', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='LinearRegression_16a5e990f186', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LinearRegression_16a5e990f186', name='labelCol', doc='label column name.'): 'Total',\n",
       " Param(parent='LinearRegression_16a5e990f186', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError',\n",
       " Param(parent='LinearRegression_16a5e990f186', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LinearRegression_16a5e990f186', name='maxIter', doc='max number of iterations (>= 0).'): 1,\n",
       " Param(parent='LinearRegression_16a5e990f186', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LinearRegression_16a5e990f186', name='regParam', doc='regularization parameter (>= 0).'): 0.5,\n",
       " Param(parent='LinearRegression_16a5e990f186', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto',\n",
       " Param(parent='LinearRegression_16a5e990f186', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LinearRegression_16a5e990f186', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3206fa3b-b369-4425-853b-91f6065bb352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2efd8531-6584-4c8c-a641-e8df2e3064ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossval = TrainValidationSplit(\n",
    "#     estimator = pipeline,\n",
    "#     estimatorParamMaps = paramGrid,\n",
    "#     evaluator = rmse_evaluator,\n",
    "#     seed = 12345678,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
